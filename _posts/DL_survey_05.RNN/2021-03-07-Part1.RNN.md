---
title:  "Deep Learning survey_7.Recurrent Neural Network (RNN)"
search: true
categories:
  - Deep learning
date: March 7, 2021
summary: This post is the seventh part (RNN) of summary of a survey paper.
toc: true
toc_sticky: true
header:
  teaser: /assets/images/thumbnails/thumb_basic.jpg
tags:
  - Deep Learning
  - RNN
last_modified_at: 2021-03-24T08:06:00-05:00
---


This post is the seventh part (RNN) of summary of a survey paper
[A state-of-the Art survey on Deep learning theory and architecture](https://www.mdpi.com/2079-9292/8/3/292).  

The following two materials were also referred to supplement the contents:  
*Deep Learning (written by Ian Goodfellow) - Book*  
[A Critical Review of Recurrent Neural Networks for Sequence Learning](https://arxiv.org/pdf/1506.00019.pdf) - paper  
The main objective of this work is to provide an overall idea on deep learning and its related fields.  


### 5.1. Introduction  

*Human thoughts have persistence*; Human don't throw a thing away and start their thinking from scratch in a second. As you are reading this article, you understand each word or sentence based on the understanding of previous words or sentences. *The traditional neural network approaches, including DNNs and CNNs cannot deal with this type of problem*.  

Much as a convolutional network is a neural network that is specialized for processing a grid of values X such as an image, a `recurrent neural network` is a neural network that is specialized for `processing a sequence of values`. Just as convolutional networks can readily scale to images with large width and height, and some convolutional networks can process images of variable size, recurrent networks can scale to much longer sequences than would be practical for networks without sequence-based specialization.  

*The standard Neural Networks and CNN are incapable due to the following reasons*.  
1. These approaches only handle `a fixed-size vector` (e.g., an image or video frame) as input and produce a fixed-size vector as output (e.g., probabilities of different classes).  
2. Those models operate with `a fixed number of computational steps` (e.g., the number of layers in the model).  

To go from multi-layer networks to recurrent networks, we need to take advantage of one of the early ideas found in machine learning and statistical models: `sharing parameters across different parts of a model`. *Parameter sharing makes it possible to extend and apply the model to examples of different forms (different lengths, here) and generalize across them.* Such sharing is particularly important when a specific piece of information can occur at multiple positions within the sequence.  
(For example, consider the two sentences "I went to Nepal in 2009" and "In 2009, I went to Nepal". If we ask a machine learning model to read each sentence and extract the year in which the narrator went to Nepal, we would like it to recognize the year 2009 as the relevant piece of information, whether it appears in the sixth word or the second word of the sentence.)  

Suppose that we trained a feedforward network that processes sentences of fixed length. A traditional fully connected feedforward network would have separate parameters for each input feature, so it would need` to learn all the rules of the language separately at each position` in the sentence. By comparison, a recurrent neural network `shares the same weights across several time steps`.  

**Recurrent network share parameter in this way:**  
Each member of the output is a function of the previous members of the output. Each member of the output is produced `using the same update rule applied to the previous outputs`.

The RNNs are unique as they allow operation over a sequence of vectors over time. The Hopfield Newark introduced this concept in 1982, the pictorial representation is shown in below figure.  

<p>
  <img src="/assets/images/blog/DL_survey_05.RNN/Figure1.png" style="width:70%">
  <figcaption>Fig.1 - The structure of basic Recurrent Neural Network with a loop. The time-unfolded computational graph (Right)</figcaption>
</p>

- At time *t*, nodes with recurrent edges receive input from the current data point *x<sup>(t)</sup>* and also from hidden node values *h<sup>(t-1)</sup>* in the network's previous state.  
- The output *<hat>y</hat><sup>(t)</sup>* (*y_hat*) at each time *t* is calculated given the hidden node values *h<sup>(t)</sup>* at time *t*.  
- Forward propagation begins with a specification of the initial state *h<sup>(0)</sup>*. Then, for each time step from *t=1* to *t=&tau;*, we apply the following update equations:  

<p>
  <img src="/assets/images/blog/DL_survey_05.RNN/Equation1.png" style="width:50%">
</p>

- The total loss for a given sequence of *x* values paired with a sequence of *y* values would then be just the sum of the losses over all the time steps. For example, if *L<sup>(t)</sup>* is the negative log-likelihood of *y<sup>(t)</sup>* given *x<sup>(1)</sup>, ..., x<sup>(t)</sup>*, then:  

<p>
  <img src="/assets/images/blog/DL_survey_05.RNN/Equation2.png" style="width:50%">
</p>

- The back-propagation algorithm applied to the unrolled graph is called **back-propagation through time** or **BPTT** and is discussed later.  

<br>

#### Early recurrent network designs  

Different versions of RNN have been proposed by Jordan and Elman.  

##### (1) RNN architecture by Jordan  

<p>
  <img src="/assets/images/blog/DL_survey_05.RNN/Figure2.png" style="width:70%">
  <figcaption>Fig.2 - A recurrent neural network as proposed by Jordan [1986]. Output units are connected to special units that at the next time step feed into themselves and into hidden units.</figcaption>
</p>

An early architecture for supervised learning on sequences was introduced by Jordan. Such a network is a feedforward network with a single hidden layer that is `extended with special units`. *Output node values are fed to the special units, which then feed these values to the hidden nodes at the following time step*. Several modern architectures use a related form of direct transfer from output nodes; `translates sentences` between natural languages, and when generating a text sequence, the word chosen at each time step is fed into the network as input at the following time step. Additionally, the special units in a Jordan network are self-connected. Intuitively, these edges allow sending information across multiple time steps without perturbing the output at each intermediate time step.  


##### (2) RNN architecture by Elman

<p>
  <img src="/assets/images/blog/DL_survey_05.RNN/Figure3.png" style="width:70%">
  <figcaption>Fig.3 - A recurrent neural network as proposed by Elman [1990]. Hidden units are connected to context units, which feed back into the hidden units at the next time step.</figcaption>
</p>

The architecture introduced by Elman is simpler than the earlier Jordan architecture. Associated with `each unit in the hidden layer is a context unit`. Each such unit *j'* takes as input the state of the corresponding hidden node *j* at the previous time step, along an edge of fixed weight *w<sub>j'j</sub>* = 1. This value then feeds back into the same hidden node *j* along a standard edge. This architecture is equivalent to a simple RNN in which each hidden node has a single self-connected recurrent edge. *The idea of fixed-weight recurrent edges that make hidden nodes self-connected is fundamental in subsequent work on LSTM networks*.  

<br>

#### Training recurrent networks  

Computing the gradient through a recurrent neural network is straightforward. No specialized algorithm are necessary. Gradients obtained by `back-propagation` may then be used with any general-purpose gradient-based techniques to train an RNN.  

To gain some intuition for how the BPTT algorithm behaves, the book (Deep learning writen by Ian Goodfellow) provides an example of how to compute gradients by BPTT for the RNN equations. For each node N we need to compute `the gradient` *&nabla;<sub>N</sub>L*, based on the gradient computed at nodes that follow it in the graph.  

I attached only the equations, you can derive the equations with some writing.  

<p>
  <img src="/assets/images/blog/DL_survey_05.RNN/Equation3.png" style="width:100%">
</p>

<br>

##### (1) Gradient Vanishing and Exploding problems  

<p>
  <img src="/assets/images/blog/DL_survey_05.RNN/Figure4.png" style="width:80%">
  <figcaption>Fig.4 - A visualization of the vanishing gradient problem. If the weight along the recurrent edge is less than one, the contribution of the input at the first time step to the output at the final time step will decrease exponentially fast as a function of the length of the time interval in between.</figcaption>
</p>

Learning with recurrent networks can be especially challenging due to `the difficulty of learning long range dependencies`. The problems of vanishing and exploding gradients occur when backpropagating errors across many time steps.  
(The tying of weights across time steps means that the recurrent edge at the hidden node j always has the same weight. Therefore, the contribution of the input at time &tau; to the output at time *t* will either explode or approach zero, exponentially fast as *t - &tau;* grows large. Hence the derivative of the error with respect to the input will either explode or vanish.)  

Which of the two phenomena occurs depends on whether the weight of the recurrent edge *<abs>w<sub>jj</sub></abs>* > 1 or *w<sub>jj</sub>* < 1 and on the activation function in the hidden node.  


##### (2) Solutions for gradient vanishing and exploding problems  

`Truncated backpropagation through time` (TBPTT) is one solution to the exploding gradient problem for continuously running networks. With TBPTT, some maximum number of time steps is set along which error can be propagated. While TBPTT with a small cutoff can be used to alleviate the exploding gradient problem, it requires that one sacrifice the ability to learn long range dependencies. The LSTM architecture uses carefully designed nodes with recurrent edges with fixed unit weight as a solution to the vanishing gradient problem.  


<br>
