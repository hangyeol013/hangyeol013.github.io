---
title:  "Deep Learning survey_8.Long Short-Term Memory (LSTM)"
search: true
categories:
  - Deep learning
summary: This post is the eighth part (LSTM) of summary of a survey paper.
last_modified_at: 2021-03-07T08:06:00-05:00
---


This post is the eighth part (LSTM) of summary of a survey paper
[A state-of-the Art survey on Deep learning theory and architecture](https://www.mdpi.com/2079-9292/8/3/292).  

The following two materials were also referred to supplement the contents:  
Book - Deep Learning (written by Ian Goodfellow)  
[A Critical Review of Recurrent Neural Networks for Sequence Learning](https://arxiv.org/pdf/1506.00019.pdf)  
Blog
[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)  
Medium - Shi Yan
[Understanding LSTM and its diagrams](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)  
The main objective of this work is to provide an overall idea on deep learning and its related fields.  



### 5.2. Long Short-Term Memory (LSTM)  


**LSTM**  

We make decisions by reasoning and by experience. (we have the feeling that our brain have a logic unit and a memory unit). So do computers, they have the logic units, CPUs and GPUs and they also have memories. Sometimes we want to remember an input for later use. There are many examples of such a situation, such as the stock market. To make a good investment judgement, we have to at least look at the stock data from a time window.  

The naive way to let neural network accept a time series data is connecting several neural networks together. Each of the neural networks handles one time step. Instead of feeding the data at each individual time step, you provide data at all time steps within a window, or a context, to the neural network.

For example, suppose you want to predict christmas tree sales. This is very seasonal thing and likely to peak only once a year. So a good strategy to predict christmas tree sale is looking at the data from exactly a year back. For this kind of problems, you either need to have a big context to include ancient data points, or you have a good memory. You know what data is valuable to remember for later use and what needs to be forgotten when it is useless.  

Theoritically the naively connected neural network (RNN) can work. But in practice, it suffers from two problems: vanishing gradient and exploding gradient, which make it unusable.  

LSTM (Long Short Term Memory) was invented to solve this issue by explicitly introducing a memory unit, called the cell into the network. This is the diagram of a LSTM building block.

<p>
  <img src="/assets/images/blog/DL_survey_05.RNN/Figure7.png" style="width:50%">
  <figcaption>Fig.1 - The diagram of a LSTM building block.</figcaption>
</p>

The network takes three inputs. *X<sub>t</sub>* is the input of the current time step. *h<sub>t-1</sub>* is the output from the previous LSTM unit and *C<sub>t-1</sub>* is the "memory" of the previous unit, which the author (Media-Shi Yan) think is the most important input. As for outputs, *h<sub>t</sub>* is the output of the current network. *C<sub>t</sub>* is the memory of the current unit.  

Therefore, this single unit makes decision by considering the current input, previous output and previous memory. And it generate a new output and alters its memory.  

The way its internal memory *C<sub>t</sub>* changes is pretty similar to piping water through a pipe. Assuming the memory is water, it flows into a pipe. Assuming the memory is water, it flows into a pipe. You want to change this memory flow along the way and this change is controlled by two valves. The first valve is called the forget valve. If you shut it, no old memory will be kept. If you fully open this valve, all old memory will pass through. The second valve is the new memory valve. New memory will come in through a T shaped joint and merge the old memory. Exactly how much new memory should come in is controlled by the second valve.  


(1) Memory pipe

<p>
  <img src="/assets/images/blog/DL_survey_05.RNN/Figure8.png" style="width:50%">
</p>

On the LSTM diagram, the top 'pipe' is the memory pipe. The input is the old memory (a vector) The first cross X it passes through is the forget valve. It is actually an element-wise multiplication operation. So if you multiply the old memory *C<sub>t-1</sub>* with a vector that is close to 0, that means you want to forget most of the old memory. You let the old memory goes through, if your forget valve equals 1.  

Then the second operation the memory flow will go through is this + operation. This operator means piece-wise summation. It resembles the T shape joint pipe. New memory and the old memory will merge by this operation. How much new memory should be added to the old memory is controlled by another valve, the X below the + sign.  

After these two operations, you have the old memory *C<sub>t-1</sub>* changed to the new memory *C<sub>t</sub>*.  


(2) Forget valve

<p>
  <img src="/assets/images/blog/DL_survey_05.RNN/Figure9.png" style="width:50%">
</p>

The forget valve is controlled by a simple one layer neural network.  
The inputs of the neural network is:  
1. *h<sub>t-1</sub>*: The output of the previous LSTM block.  
2. *X<sub>t</sub>*: The input for the current LSTM block.  
3. *C<sub>t-1</sub>*: The memory of the previous block.  
4. *b<sub>0</sub>*: A bias vector  
This neural network has a sigmoid function as activation, and it's output vector is forget valve, which will applied to the old memory *C<sub>t-1</sub>* by element-wise multiplication.  


(3) Memory valve  

<p>
  <img src="/assets/images/blog/DL_survey_05.RNN/Figure10.png" style="width:50%">
</p>

Again the memory valve is a one layer simple neural network that takes the same inputs as the forget valve. This valve controls how much the new memory should influence the old memory. The new memory itself, however is generated by another neural network. It is also a one layer network, but uses tanh as the activation function. The output of this network will element-wise multiple the new memory valve, and add to the old memory to form the new memory.  

**Why do we need the left sub unit (4 inputs with sigmoid function)????**
A sigmoid layer called the 'input gate layer' decides which values we'll update.  
A tanh layer creates a vector of new candidate values.  


<p>
  <img src="/assets/images/blog/DL_survey_05.RNN/Figure11.png" style="width:50%">
</p>

These two X sings are the forget valve and the new memory valve.  


(4) Output valve  

<p>
  <img src="/assets/images/blog/DL_survey_05.RNN/Figure12.png" style="width:50%">
</p>

We need to generate the output for this LSTM unit. This step has an output valve that is controlled by:  
1. *C<sub>t</sub>*: The new memory  
2. *h<sub>t-1</sub>*: The previous output
3. *X<sub>t</sub>*: A bias vector  
This valve controls how much new memory should output to the next LSTM unit.  


<br>
