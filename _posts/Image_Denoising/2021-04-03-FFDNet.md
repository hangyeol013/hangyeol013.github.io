---
title:  "FFDNet paper_Review"
search: true
categories:
  - Image denoising
date: April 02, 2021
summary: This post is a summary of FFDNet (Fast and Flexible Denoising CNN) paper.
toc: true
toc_sticky: true
header:
  teaser: /assets/images/thumbnails/thumb_basic.jpg
tags:
  - Deep Learning
  - Image denoising
last_modified_at: 2021-04-03T08:06:00-05:00
---


This post is a summary of a FFDNet (Fast and Flexible Denoising CNN) paper  
[FFDNet: Toward a Fast and Flexible Soultion for CNN based Image Denoising](https://arxiv.org/pdf/1710.04026.pdf).  

<br>


### 1. Introduction  

In order to handle practical image denoising problems, a flexible image denoiser is expected to have the following desirable properties:  
1. it is able to perform denoising using a single model  
2. It is efficient, effective and user-friendly  
3. It can handle spatially variant noise  
Such a denoiser can be directly deployed to recover the clean image when the noise level is known or can be well estimated.  

In general, image denoising methods can be grouped into two major categories, model-based methods and discriminative learning based methods.  

**Model-based methods**  
- BM3D, WNNM  
- Flexible in handling denoising problems with various noise levels
- Optimization algorithms are generally time-consuming  
- Cannot be directly used to remove spatially variant noise  
- Usually employ hand-crafted image priors, which may not be strong enough to characterize complex image structures  

**Discriminative learning-based methods**  
- MLP, DnCNN  
- Aim to learn the underlying image prior  
- fast inference from a training set of degraded and ground-truth image pairs  
- Limited in flexibility (lack flexibility to deal with spatially variant noise)  
- Learned model is usually tailored to specific noise level  
- Even DnCNN-B cannot generalize well to real noisy images (work only in the present range [0, 55])  

#### * Difference between DnCNN and FFDNet  

**DnCNN)**  
- Formulated as x = F(y; &theta;<sub>&sigma;</sub>)
- The parameter &theta;<sub>&sigma;</sub> vary with the change of noise level &sigma;  

**FFDNet)**  
- Formulated as x = F(y,M; &theta;)
- M is a noise level maps  
- The noise level map is modeled as an input and the model parameters &theta; are invariant to noise level  
- Thus, FFDNet provides a flexible way to handle different noise levels with a single network  

By introducing a noise level map as input, it is natural to expect that the model performs well when the noise level map matches the ground-truth one of noisy input.  

Furthermore, the noise level map should also play the role of controlling the trade-off between noise reduction and detail preservation. It is found that heavy visual quality degradation may be engendered when setting a larger noise level to smooth out the details. The authors adopt a method of orthogonal initialization on conventional filters to alleviate this problem.  

Besides, the proposed FFDNet works on downsampled sub-images, which largely accelerates the training and testing speed, and enlarges the receptive field as well.  

The main contribution of their work is sumarized as follows:  
1. By taking a tunable noise level map as input, a single FFDNet is able to deal with noise on different levels, as well as spatially variant noise.  
2. They highlight the importance to guarantee the role of the noise level map in controlling the trade-off between noise reduction and detail preservation.  
3. FFDNet exhibits perceptually appealing results, demonstrating its potential for practical image denoising.  

<br>

### 2. Related Work    


#### A. MAP Inference Guided Discriminative Learning  

Here, I reviewed about MAP (Maximum A posteriori) comparing with MLE (Maximum Likelihood Estimation). I referred to the content from Shota Horii's post, you can access and read more in detail on his post:  
[A Gentle Introduction to Maximum Likelihood Estimation and Maximum A Posteriori Estimation](https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-and-maximum-a-posteriori-estimation-d7c318f9d22d)  

MLE and MAP estimation are method of estimating parameters of statistical models.  

With the assumption that probability &theta; follows binomial distribution, The formula of the probability is below:  

<p>
  <img src="/assets/images/blog/Image_Denoising/Equation2.png" style="width:70%">
</p>

I took this equation from his post, so it's expressed with 'k wins out of n matches', which is a example in that post.  
This simplification is the statistical modelling of the example, and &theta; is the parameter to be estimated with MLE and MAP.  



**MLE (Maximum Likelihood Estimation)**  

Likelihood: *P(D|&theta;)*  
'MLE' aims to solve 'What is the exact value of &theta; which maximize the likelihood *P(D|&theta;)*?'  
The value of &theta; maximizing the likelihood can be obtained by having derivative of the likelihood function with respect to &theta;, and setting it to zero.  

<p>
  <img src="/assets/images/blog/Image_Denoising/Equation3.png" style="width:70%">
</p>

By solving this, &theta; = 0,1 or k/n.


**MAP (Maximum A Posteriori Estimation)**  

'MLE' is powerful when you have enough data. However, it doesn't work well when observed data size is small. However, when you have a prior knowledge, it can be helpful to estimate the parameters. This prior knowledge is called 'prior probability', P(&theta;).  

Then, the updated probability of &theta; given D (observed data) is expressed as *P(&theta;\D)* and called the `posterior probability`.  

Now, we want to know the best guess of &theta; considering both our prior knowledge and the observed data. It means maximizing *P(&theta;\D)* and it's the MAP estimation. 


<p>
  <img src="/assets/images/blog/Image_Denoising/Equation4.png" style="width:20%">
</p>
Here, we can calculate *P(&theta;|D)* using Bayes' theorem below.  
<p>
  <img src="/assets/images/blog/Image_Denoising/Equation5.png" style="width:30%">
</p>
P(D) is independent to the value of &theta; and since we're only interested in finding &theta; maximizing *P(&theta;|D)*, we can ignore *P(D)* in the maximization.  
<p>
  <img src="/assets/images/blog/Image_Denoising/Equation6.png" style="width:40%">
</p>

Intrinsically, we can use any formulas describing probability distribution as *P(&theta;)* to express the prior knowledge well. However, for the computational simplicity, specific probability distributions are used corresponding to the probability distribution of likelihood. It's called `conjugate prior distribution`.  

Since the conjugate prior of binomial distribution is Beta distribution, we use Beta distribution to express *P(&theta;)* here.  
<p>
  <img src="/assets/images/blog/Image_Denoising/Equation7.png" style="width:40%">
</p>
Where, &alpha; and &beta; are called hyperparameter, which cannot be determined by data. Rather we set them subjectively to express our prior knowledge well.  

So, by now we have all the components to calculate *P(D|&theta;)P(&theta;)* to maximize.  
<p>
  <img src="/assets/images/blog/Image_Denoising/Equation8.png" style="width:60%">
</p>

As same as MLE, we can get &theta; maximizing this by having derivative of the this function with respect to &theta;, and setting it to zero.  


MAP inference guided discriminative learning usually requires much fewer inference steps, and is very efficient in image denoising. It also has clear interpretability because the discriminative architecture is derived from optimization algorithms.  

However, the learned priors and inference procedure are limited by the form of MAP model, and generally perform inferior to the state-of-the-art CNN-based denoisers.  


#### B. Plain Discriminative Learning  

Instead of modeling image priors explicitly, the plain discriminative learning methods learn a direct mapping function to model image prior implicitly.  

Plain discriminative learning has shown better performance than MAP inference guided discriminative learning. However, existing discriminative learning methods have to learn multiple models for handling images with different noise levels, and are incapable to deal with spatially variant noise.  

It remains an unaddressed issue to develop a single discriminative denoising model which can handle noise of different levels, even spatially variant noise, in a speed even faster than BM3D.  

<br>

### 3. Proposed Fast and Flexible Discriminative CNN Denoiser  





<p>
  <img src="/assets/images/blog/Image_Denoising/Figure1.png" style="width:100%">
  <figcaption>
  Fig.1 - The architecture of the proposed DnCNN network.
  </figcaption>
</p>
