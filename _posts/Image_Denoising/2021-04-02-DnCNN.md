---
title:  "DnCNN paper_Review"
search: true
categories:
  - Image denoising
date: April 02, 2021
summary: This post is a summary of DnCNN (image denoising using CNN) paper.
toc: true
toc_sticky: true
header:
  teaser: /assets/images/thumbnails/thumb_basic.jpg
tags:
  - Deep Learning
  - Image denoising
last_modified_at: 2021-04-02T08:06:00-05:00
---


This post is a summary of a DnCNN (image denoising using CNN) paper
[Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising](https://arxiv.org/pdf/1608.03981.pdf).  

### Abstract  

Discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, the authors take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, `residual learning` and `batch normalization` are utilized to speed up the training process as well as boost the denoising performance.  

Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise (AWGN) at a certain noise level, DnCNN model is able to handle `Gaussian denoising with unknown noise level` (i.e., blind Gaussian denoising).  

*Additive white Gaussian noise (AWGN))
- Additive: because it is added to any noise that might be intrinsic to the information system.  
- White: It has uniform power across the frequency band for the information system.  
- Gaussian: It has a normal distribution in the time domain with an average time domain value of zero.  

<br>

### 1. Introduction  

The goal of image denoising is to recover a clean image *x* from a noisy observation *y* which follows an image degradation model y = x + v. One common assumption is that v is additive white Gaussian noise (AWGN) with standard deviation &sigma;. From a Bayesian viewpoint, when the likelihood is known, the image prior modeling will play a central role in image denoising. In particular, the NSS (Non self-similarity) models are popular in state-of-the-art methods such as BM3D, LSSC, NCSR and WNNM.  
Despite their high denoising quality, most of the image prior-based methods typically suffer from two major drawbacks.  
1. Those methods generally involve a complex optimization problem in the testing stage, making the denoising process time-consuming.  
&rarrw; Most of the prior-based methods can hardly achieve high performance without sacrificing computational efficiency.  
2. The models in general are non-convex and involve several manually chosen parameters, providing some leeway to boost denoising performance.  

To overcome the limitations of prior-based approaches, several discriminative learning methods (CSF, TNRD) have been recently developed to learn image prior models in the context of truncated inference procedure. The resulting models are able to get rid of the iterative optimization procedure in the test phase. Although CSF and TNRD have shown promising results toward bridging the gap between computational efficiency and denoising quality, their performance are inherently restricted to the specified forms of prior. To be specific, the priors adopted in CSF and TNRD are based on the analysis model, which is limited in capturing the full characteristics of image structures. Another nonnegligible drawback is that they train a specific model for a certain noise level, and are limited in blind image denoising.  

In this paper, instead of learning a discriminative model with an explicit image prior, the authors treat image denoising as a plain discriminative learning problem. (i.e., separating the noise from a noisy image by feed-forward convolutional neural networks).  

The reasons of using CNN are three-fold:  
1. CNN with very deep architecture is effective in increasing the capacity and flexibility for exploiting image characteristics.  
2. Considerable advances have been achieved on regularization and learning methods for training CNN, including ReLU, BN and residual learning.
&rarrw; these methods can be adopted in CNN to speed up the training process and improve the performance.   
3. CNN is well-suited for parallel computation on modern powerful GPU.  

Rather than directly outputting the denoised image x&#770;, the proposed DnCNN is designed to predict the residual image v&#770;, (i.e., the difference between the noisy observation and the latent clean image). In other words, the proposed DnCNN implicitly removes the latent clean image with the operations in the hidden layers. The batch normalization technique is further introduced to stabilize and enhance the training performance of DnCNN. It turns out that residual learning and batch normalization can benefit from each other, and their integration is effective in speeding up the training and boosting the denoising performance.  

The contributions of this work are summarized as follows:  
1. They propose an end-to-end trainable deep CNN for Gaussian denoising. In contrast to the existing dnn-based methods, DnCNN adopts the residual learning strategy.  
2. They find that residual learning and batch normalization can greatly benefit the CNN learning (training speed, performance)  
3. DnCNN can be easily extended to handle general image denoising tasks. (blind Gaussian denoising, SISR, JPEG deblocking)  

+) The first CNN model for general image denoising (not for a certain noise level)  

<br>

### 2. The proposed Denoising CNN Model  

In this section, we'll look the proposed denoising CNN model (DnCNN, and extend it for handling several general image denoising tasks).
- Network architecture design: modified VGG network to make it suitable for image denoising.  
- Model learning: Residual learning, batch normalization.  


#### A. Network Depth  

- Convolutional filter size: 3 x 3  
- Pooling layer: all removed  
- Receptive field: (2*d*+1) x (2*d*+1)  
For better tradeoff between performance and efficiency, one important issue in architecture design is to set a proper depth for DnCNN.  
The receptive field size of denoising NN correlates with the effective patch size of denoising methods. Moreover, high noise level usually requires larger effective patch size to capture more context information for restoration.  
The receptive field size and the number of layers of DnCNN for Gaussian denoising with  
- a certaion noise level: 35x35 (17 layers)  
- general image denoising: 61x61 (20 layers)  


#### B. Network Architecture  

The input of DnCNN: A noisy observation *y* = *x* + v.  
The loss function: the averaged MSE between the desired residual images and estimated ones,  
<p>
  <img src="/assets/images/blog/Image_Denoising/Equation1.png" style="width:60%">
</p>
Here *{(y<sub>i</sub>, x<sub>i</sub>)}<sup>N</sup><sub>i=1</sub>* represents N noisy-clean training image (patch) pairs.


Figure.1 illustrates the architecture of the proposed DnCNN for learning *R*(y).  
<p>
  <img src="/assets/images/blog/Image_Denoising/Figure1.png" style="width:100%">
  <figcaption>
  Fig.1 - The architecture of the proposed DnCNN network.
  </figcaption>
</p>


##### 1) Deep Architecture:  
There are 3 types of layers:  
1. Conv + ReLU (For the first layer): 64 filters of size 3 x 3 x c (# of image channels)  
2. Conv + BN + ReLU (for layers 2~(D-1)): 64 filters of size 3 x 3 x 64  
3. Conv (for the last layer): c filters of size 3 x 3 x 64  

To sum up, the DnCNN model has two main features: the residual learning formulation and batch normalization. By incorporating convolution with ReLU, DnCNN can gradually separate image structure from the noisy observation through the hidden layer.  


##### 2) Reducing Boundary Artifacts:  

In many low level vision applications, it usually requires that the output image size should keep the same as the input one. This may lead to the boundary artifacts.  
- Previous methods: symmetrically padded in the preprocessing stage. (MLP, CDF, TNRD)  
- DnCNN: pad zeros before convolution (each feature map of the middle layers has the same size as the input image)  

<br>

#### C. Integration of Residual Learning and Batch Normalization for Image Denoising  

Figure.2 shows the average PSNR values obtained using the two learning formulations (y &rarrw; x (original mapping) or y &rarrw; v (residual mapping)) with/without batch normalization under the same setting on gradient-based optimization algorithm and network architecture.  
<p>
  <img src="/assets/images/blog/Image_Denoising/Figure2.png" style="width:100%">
  <figcaption>
  Fig.2 - The Gaussian denoising results of four specific models under two gradient-based optimization algorithms (a) SGD, (b) Adam, with respect to epochs. The four specific models are trained with noise level 25. The results are evaluated on 68 natural images from Berkeley segmentation dataset.
  </figcaption>
</p>
1. The residual learning formulation can result in faster and more stable convergence than the original mapping learning.  
2. Without BN, simple residual with conventional SGD cannot compete with the SOTA denoising methods. (TNRD: 28.92dB)  
3. With BN, learning residual mapping converges faster with better performance.  

It is the integration of residual learning formulation and BN rather than the optimization algorithms (SGD or Adam) that leads to the best denoising performance.  

With residual learning, DnCNN implicitly removes the latent clean image with the operations in the hidden layers. This makes that the inputs of each layer are Gaussian-like distributed, less correlated, and less related with image content. Thus, residual learning can also help BN in reducing internal covariate shift.  


#### D. Connection with TNRD  
