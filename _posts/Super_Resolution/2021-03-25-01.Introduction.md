---
title:  "Super Resolution Survey 1. Introduction"
search: true
categories:
  - Super Resolution
date: March 25, 2021
summary: This post is the first part of the summary of SR survey paper.
toc: true
toc_sticky: true
header:
  teaser: /assets/images/thumbnails/thumb_basic.jpg
tags:
  - Deep Learning
  - Super Resolution
last_modified_at: 2021-03-24T08:06:00-05:00
---

The main objective of this work is to provide an overall idea on super resolution and its related models. Based on the flow of this survey paper, I supplemented some explanations by referring to other articles or papers. This post is just a summary of the summary paper below, so if you want to look into the contents in detail, you can click the links I left.  

[Deep Learning for Image Super-Resolution: A Survey](https://arxiv.org/pdf/1902.06068.pdf).  

<br>

**Super Resolution:**  
The process of recovering high-resolution (HR) images from low-resolution (LR) images.  

### 1. Introduction  

It enjoys a wide range of real-world applications, such as medical imaging, surveillance and security, amongst others. In general, this problem is very challenging and inherently ill-posed since there are always multiple HR images corresponding to a single LR image.  

With the rapid development of deep learning techniques in recent years, deep learning based SR models have been actively explored and often achieve the state-of-the-art performance on various benchmarks of SR, ranging from the early Convolutional Neural Networks (CNN) based method to recent promising SR approaches using Generative Adversarial Nets (GAN).  

In general, the family of SR algorithms using deep learning techniques differ from each other in the following major aspects:  
1. Different types of network architectures  
2. Different types of loss functions  
3. Different types of learning principles and strategies  

In the following sections, we will cover various aspects of recent advances in image super resolution with deep learning. Fig.1 shows the taxonomy of image SR to be covered in this survey in a hierarchically-structured way.  

<p>
  <img src="/assets/images/blog/Super_Resolution/Figure1.png" style="width:100%">
  <figcaption>Fig.1 - Hierarchically-structured taxonomy of this survey.</figcaption>
</p>


### 2. Problem Setting and Terminology    

#### (1) Problem definitions  

Image super-resolution aims at recovering the corresponding HR images from the LR images. Generally, the LR image *I<sub>x</sub>* is modeled as the output of the following degradation:  

<p>
  <img src="/assets/images/blog/Super_Resolution/Equation1.png" style="width:15%">
</p>

where *D* denotes a degradation mapping function *I<sub>y</sub>* is the corresponding HR image and &delta; is the parameters of the degradation process. (e.g., the scaling factor or noise) Generally, the degradation process is unknown and only LR images are provided. In this case, researchers are required to recover an HR approximation *<hat>I</hat><sub>y</sub>* (I_hat) of the ground truth HR image *I<sub>y</sub>* from the LR image *I<sub>x</sub>*, following:  

<p>
  <img src="/assets/images/blog/Super_Resolution/Equation2.png" style="width:15%">
</p>

where *F* is the super-resolution model and &theta; denotes the parameters of *F*.  

Most works directly model the degradation as a single downsampling operation. As a matter of fact, most datasets for generic SR are built based on this pattern, and the most commonly used downsampling operation is bicubic interpolation with anti-aliasing.  

To this end, the objective of SR is as follows:  

<p>
  <img src="/assets/images/blog/Super_Resolution/Equation3.png" style="width:40%">
</p>

where *L(<hat>I</hat><sub>y</sub>, I<sub>y</sub>)* represents the loss function between the generated HR images *<hat>I</hat><sub>y</sub>* (I_hat) and the ground truth image *I<sub>y</sub>*, &Phi;(&theta;) is the regularization term and &lambda; is the tradeoff parameter. Although the most popular loss function for SR is pixel-wise mean squared error, more powerful models tend to use a combination of multiple loss functions.  


#### (2) Dataset for Super-resolution    

Today there are a variety of datasets available for image super-resolution, which greatly differ in image amounts, quality, resolution, and diversity, etc. Some of them provide LR-HR image pairs, while others only provide HR images, in which case the LR images are typically obtained by *imresize* function with default setting in MATLAB (i.e., bicubic interpolation with anti-aliasing). In table 1 they list a number of image datasets commonly used by the SR community.

<p>
  <img src="/assets/images/blog/Super_Resolution/Table1.png" style="width:100%">
  <figcaption>Table.1 - List of public image datasets for super-resolution benchmarks.</figcaption>
</p>


#### (3) Image Quality Assessment  

Image quality refers to visual attributes of images and focuses on the perceptual assessments of viewers. In general, image quality assessment (IQA) methods include subjective methods based on human's perception (i.e., how realistic the image looks) and objective computational methods. The former is more in line with our need but often time-consuming and expensive, thus the latter is currently the mainstream. However, these methods aren't necessarily consistent between each other, because objective methods are often unable to capture the human visual perception very accurately, which may lead to large difference in IQA results.  

The objective IQA methods are further divided into three types: full-reference methods performing assessment using reference images, reduced-reference methods based on comparisons of extracted features, and no-reference methods (i.e., blind IQA) without any reference images.  

Let's see several most commonly used IQA methods covering both subjective methods and objective methods.  


##### 3.1 Peak Signal-to-Noise Ratio  

Peak signal-to-noise ratio (PSNR) is one of the most popular reconstruction quality measurement of lossy transformation. For image super resolution, PSNR is defined via the maximum pixel value (denoted as L) and the mean squared error (MSE) between images. Given the ground truth image *I* with *N* pixels and the reconstruction *<hat>I</hat>* (I_hat), the PSNR between *I* and *<hat>I</hat>* (I_hat) are defined as follows:  

<p>
  <img src="/assets/images/blog/Super_Resolution/Equation4.png" style="width:40%">
</p>

where L equals to 255 in general cases using 8-bit representations.  

Since the PSNR is only related to the pixel-level MSE, only caring about the differences between corresponding pixels instead of visual perception, it often leads to poor performance in representing the reconstruction quality in real scenes, where we're usually more concerned with human perceptions. However, due to the necessity to compare with literature works and the lack of completely accurate perceptual metrics, PSNR is still currently the most widely used evaluation criteria for SR models.  


##### 3.2 Structural Similarity  

Considering that the human visual system (HVS) is highly adapted to extract image structures, the structural similarity index (SSIM) is proposed for measuring the structural similarity between images, based on independent comparisons in terms of luminance, contrast, and structures.  

For an image *I* with *N* pixels, the luminance &mu;<sub>*I*</sub> and contrast &sigma;<sub>*I*</sub> are estimated as the mean and standard deviation of the image intensity, respectively. *I(i)* represents the intensity of the *i*-th pixel of image *I*. And the comparisons on luminance and contrast, denoted as *C<sub>l</sub>(I, I_hat)* and *C<sub>c</sub>(I, I_hat)* respectively, are given by:  

<p>
  <img src="/assets/images/blog/Super_Resolution/Equation5.png" style="width:40%">
</p>

The structure comparison function, *C<sub>s</sub>(I, I_hat)* is denoted as:  

<p>
  <img src="/assets/images/blog/Super_Resolution/Equation6.png" style="width:40%">
</p>

where &sigma;*<sub>I,I_hat</sub>* is the covariance between *I* and *<hat>I</hat>* (I_hat), and *C<sub>3</sub>* is a constant for stability.  

Finally, the SSIM is given by:  

<p>
  <img src="/assets/images/blog/Super_Resolution/Equation7.png" style="width:40%">
</p>

where &alpha;, &beta;, &gamma; are control parameters for adjusting the relative importance.  

Since the SSIM evaluates the reconstruction quality from the perspective of the HVS, it better meets the requirements of perceptual assessment, and is also widely used.  


##### 3.3 Mean Opinion Score  

Mean Opinion Score (MOS) testing is a commonly used subjective IQA method, where human raters are asked to assign perceptual quality scores to tested images. Typically, the score are from 1 (bad) to 5 (good). And the final MOS is calculated as the arithmetic mean over all ratings.  
Although the MOS testing seems a faithful IQA method, it has some inherent defects, such as non-linearly perceived scales, biases and variance of rating criteria. In reality, there are some SR models performing poorly in common IQA metrics (e.g., PSNR) but far exceeding others in terms of perceptual quality, in which case the MOS testing is the most reliable IQA method for accurately measuring the perceptual quality.  


##### 3.4 Learning-based Perceptual Quality  

In order to better assess the image perceptual quality while reducing manual intervention, researchers try to assess the perceptual quality by learning on large datasets. Specifically, no-reference Ma and NIMA were proposed, which are learned from visual perceptual scores and directly predict the quality scores without ground-truth images. Also, there were several learning based perceptual quality methods (DeepQA, LPIPS).  
Although these methods exhibit better performance on capturing human visual perception, what kind of perceptual quality we need (e.g., more realistic images, or consistent identity to the original image) remains a question to be explored, thus the objective IQA methods (e.g., PSNR, SSIM) are still the mainstreams currently.  


##### 3.5 Task-based Evaluation  

According to the fact that SR models can often help other vision tasks, evaluating reconstruction performance by means of other tasks is another effective way. Specifically, researchers feed the original and the reconstructed HR images into trained models, and evaluate the reconstruction quality by comparison by comparing the impacts on the prediction performance. The vision tasks used for evaluation include object recognition, face recognition, face alignment and parsing, etc.  


##### 3.6 Other IQA Methods  

In addition to above IQA methods, there are other less popular SR metrics.

- MS-SSIM (Multi-scale SSIM): More flexibility than single-scale SSIM in incorporating the variations of viewing conditions.  
- FSIM (Feature Similarity): extracts feature points of human interest based on phase congruency and image gradient magnitude to evaluate image quality.  
- NIQE (Natural Image Quality Evaluator): makes use of measurable deviations from statistical regularities observed in natural images, without exposure to distorted images.  

Recently, Blau et al. prove mathematically that distortion (e.g., PSNR, SSIM) and perceptual quality (e.g., MOS) are at odds with each other, and show that as the distortion decreases, the perceptual quality must be worse. Thus, how to accurately measure the SR quality is still an urgent problem to be solved.  



#### (4) Operating Channels  

In addition to the commonly used RGB color space, the YCbCr color space is also widely used for SR. In this space, images are represented by Y, Cb, Cr channels, denoting the luminance, blue-difference and red-difference chroma components, respectively. Although currently, there is no accepted best practice for performing or evaluating super-resolution on which space, earlier methods favor operating on the Y channel of YCbCr space, while more recent models tend to operate on RGB channels. It is worth noting that operating (training or evaluation) on different color spaces or channels can make the evaluation results differ greatly (up to 4dB).  


<br>


In this post, we looked over an overview of the super resolution. In the next post, we will look into the Super resolution in more detail.
