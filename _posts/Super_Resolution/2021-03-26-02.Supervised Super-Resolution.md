---
title:  "Super Resolution Survey 2. Supervised Super-Resolution"
search: true
categories:
  - Super Resolution
date: March 26, 2021
summary: This post is the second part of the summary of SR survey paper.
toc: true
toc_sticky: true
header:
  teaser: /assets/images/thumbnails/thumb_basic.jpg
tags:
  - Deep Learning
  - Super Resolution
last_modified_at: 2021-03-26T08:06:00-05:00
---

The main objective of this work is to provide an overall idea on super resolution and its related models. Based on the flow of this survey paper, I supplemented some explanations by referring to other articles or papers. This post is just a summary of the summary paper below, so if you want to look into the contents in detail, you can click the links I left.  

[Deep Learning for Image Super-Resolution: A Survey](https://arxiv.org/pdf/1902.06068.pdf).  

<br>

I looked over the overview of super resolution in the last post, here I will look into the super resolution in more detail.    

### 3. Supervised Super-Resolution  

Nowadays researchers have proposed a variety of super-resolution models with deep learning. These models focus on supervised SR, i.e., trained with both LR images and corresponding HR images. Although the differences between these models are very large, they are essentially some combinations of a set of components such as `model frameworks`, `upsampling methods`, `network design`, and `learning strategies`. From this perspective, researchers combine these components to build an integrated SR model for fitting specific purposes. In this section, the authors concentrated on modularly analyzing the fundamental components and summarizing their advantages and limitations.  


#### (1) Super-resolution Frameworks  

Since image super-resolution is an ill-posed problem, how to perform upsampling is the key problem. The architectures of existing models can be attributed to four model frameworks, based on the employed upsampling operations and their locations in the model.    

<p>
  <img src="/assets/images/blog/Super_Resolution/Figure2.png" style="width:100%">
  <figcaption>Fig.1 - Super-resolution frameworks based on deep learning. The gray one denote predefined upsampling, while the green, yellow and blue ones indicate learnable upsampling, downsampling and convolutional layers, respectively. And the blocks enclosed by dashed boxes represent stakable modules.</figcaption>
</p>

##### 1.1 Pre-upsampling Super-resolution  

On account of the difficulty of directly learning the mapping from low-dimensional space to high-dimensional space, utilizing `traditional upsampling algorithms` to obtain higher-resolution images and then refining them using deep neural networks is a straightforward solution. (e.g., SRCNN). Specifically, the LR images are upsampled to coarse HR images with the desired size using traditional methods (e.g., bicubic interpolation), then deep CNNs are applied on these images for reconstructing high-quality details.  

- Since the most difficult upsampling operation has been completed, the learning difficulty is significantly reduced.  
- The predefined upsampling often introduce side effects (e.g., noise amplification and blurring).  
- Since mose operations and performed in high-dimensional space, the cost of time and space is much higher than other frameworks.  


##### 1.2 Post-upsampling Super-resolution  

In order to improve the computational efficiency and make full use of deep learning technology to increase resolution automatically, researchers propose to perform most computation in low-dimensional space by replacing the predefined upsampling with ene-to-end learnable layers integrated at the end of the models. Specifically, the LR input images are fed into deep CNNs without increasing resolution, and end-to-end learnable upsampling layers are applied to the end of the network.  
- The computation and spatial complexity are much reduced. (Feature extraction process only occurs in low-dimensional space)  
- This framework has been one of the most mainstream framework.  
- These models differ mainly in the learnable upsampling layers, anterior CNN structures and learning strategies.  


##### 1.3 Progressive Upsampling Super-resolution  

Although post-upsampling SR framework has immensely reduced the computational cost, it still has some shortcomings:  
1. The upsampling is performed in only one step, which greatly increases the learning difficulty for large scaling factors (e.g., 4, 8).
2. Each scaling factor requires training an individual SR model, which cannot cope with the need for multi-scale SR.  

To address these drawbacks, a progressive upsampling framework is adopted by LapSRN. Specifically, the models under this framework are based on a cascade of CNNs and progressively reconstruct higher-resolution images. At each stage, the images are upsampled to higher resolution and refined by CNNs. (e.g., LapSRN, MS-LapSRN, ProSR)  
- This framework greatly reduce the learning difficulty, especially with large factors.  
- Also cope with the multi-scale SR without introducing overmuch spacial and temporal cost.
- Some specific learning strategies such as curriculum learning and multi-supervision can be directly integrated to further reduce learning difficulty and improve final performance.  
- The complicated model designing for multiple stages and the training stability.  
- More modeling guidance and more advanced training strategies are needed.  


##### 1.4 Iterative Up-and-down Sampling Super-resolution  

In order to better capture the nutual dependency of LR-HR image pairs, an efficient iterative procedure named back-projection is incorporated into SR. This SR framework tries to iteratively apply back-projection refinement (i.e., computing the reconstruction error then fusing it back to tune the HR image intensity) back to tune the HR image intensity. (e.g., DBPN, SRFBN, RBPN)

- Provide higher-quality reconstruction results. (This framework can better mine the deep relationships between LR-HR image pairs)  
- The design criteria of the back-projection modules are still unclear (Since this mechanism has just been introduced into deep learning-based SR, the framework has great potential and needs further exploration)  

<br>

#### (2) Upsampling Methods  

In addition to the upsampling positions in the model, how to perform upsampling is of great importance. Although there has been various traditional upsampling methods, making use of CNNs to learn end-to-end upsampling has gradually become a trend. In this section, the authors introdued some traditional interpolation-based algorithms and deep learining-based upsampling layers.  


##### 2.1 Interpolation-based upsampling  

Image interpolation (a.k.a. image scaling) refers to resizing digital images and is widely used by image-related applications. The traditional interpolation methods include nearest-neighbor interpolation, bilinear and bicubic interpolation, Sinc and Lanczos resampling, etc. Since these methods are interpretable and easy to implement, some of them are still widely used in CNN-based SR models.  

<p>
  <img src="/assets/images/blog/Super_Resolution/Figure3.png" style="width:100%">
  <figcaption>Fig.2 - Comparison of 1D and 2D interpolation of NN-interpolation, BLI and BCI.</figcaption>
</p>

**Nearest-neighbor Interpolation**  
It selects the value of the nearest pixel for each position to be interpolated regardless of any other pixels. Thus this method is very fast but usually produces blocky results of low quality.  

**Bilinear Interpolation**  
The bilinear interpolation (BLI) first performs linear interpolation on one axis of the image and then performs on the other axis. Since it results in a quadratic interpolation with a receptive field sized 2x2, it shows much better performance than nearest-neighbor interpolation while keeping relatively fast speed.  

**Bicubic Interpolation**  
The bicubic interpolation (BCI) performs cubic interpolation on each of the two axes. Compared to BLI, the BCI takes 4x4 pixels into account, and result in smoother results with fewer artifacts but much lower speed. In fact, the BCI with anti-aliasing is the mainstream method for building SR datasets, and is also widely used in pre-upsampling SR framework.  

As a matter of fact, the interpolation-based upsampling methods improve the image resolution only based on its own image signals, without bringing any more information. Instead, they often introduce some side effects, such as computational complexity, noise amplification, blurring results. Therefore, the current trend is to replace the interpolation-based methods with learnable upsampling layers.  


#### 2.2 Learning-based Upsampling  

In order to overcome the shortcomings of interpolation based methods and learn upsampling in an end-to-end manner, transposed convolution layer and sub-pixel layer are introduced into the SR field.  

**Transposed Convolution Layer**  
Transposed convolution layer (a.k.a. deconvolution layer) tries to perform transformation opposite a normal convolution, (i.e., predicting the possible input based on feature maps sized like convolution output.) Specifically, it increases the image resolution by expanding the image by inserting zeros and performing convolution.  

<p>
  <img src="/assets/images/blog/Super_Resolution/Figure4.png" style="width:100%">
  <figcaption>Fig.3 - Transposed convolution layer. The blue boxes denote the input and the green boxes indicate the kernel and the convolution output.
  </figcaption>
</p>

Since the transposed convolution enlarges the image size in an end-to-end manner while maintaining a connectivity pattern compatible with vanilla convolution, it is widely used as upsampling layers in SR models. However, this layer can easily cause "uneven overlapping" on each axis, and the multiplied results on both axes further create a checkboard-like pattern of varying magnitudes and thus hurt the SR performance.  
(*Vanilla network: 'vanilla' is a common euphemism for 'regular' or 'without any fancy stuff'. It's used by Hastie in their book "The Elements of Statistical Learning" to mean a feed-forward network with a single hidden layer, the most basic of the commonly used network types.)  


**Sub-pixel Layer**  
The sub-pixel layer performs upsampling by generating a plurality of channels by convolution and then reshaping them.

<p>
  <img src="/assets/images/blog/Super_Resolution/Figure5.png" style="width:100%">
  <figcaption>
  Fig.4 - Sub-pixel layer. The blue boxes denote the input, and the boxes with other colors indicate different convolution operations and different output feature maps. With this layer, a convolution is firstly applied for producing outputs with *s<sup>2</sup>* times channels, where *s* is the scaling factor. Assuming the input size is *h* x *w* x *c*, the output size will be *h* x *w* x *s<sup>2</sup>c*. After that, the reshaping operation (a.k.a. *shuffle*) is performed to produce outputs with size *sh* x *sw* x *c*. In this case, the receptive field can be up to 3x3.
  </figcaption>
</p>

Due to the end-to-end upsampling manner, this layer is also widely used by SR models. Compared with transposed convolution layer, the sub-pixel layer has a larger receptive field, which provides more contextual information to help generate more realistic details. However, since the distribution of the receptive fields is uneven and blocky regions actually share the same receptive field, it may result in some artifacts near the boundaries of different blocks. On the other hand, independently predicting adjacent pixels in a blocky region may cause unsmooth outputs. Thus, Gao propose PixelTCL, which replaces the independent prediction to interdependent sequential prediction, and produces smoother and more consistent results.  


**Meta Upscale Module**  
The previous methods need to predefine the scaling factors (i.e., training different upsampling modules for different factors), which is inefficient and not in line with real needs. So that [Hu et al.](https://arxiv.org/pdf/1903.00875.pdf) propose meta upscale module, which firstly solves SR of arbitrary scaling factors based on meta learning.  

<p>
  <img src="/assets/images/blog/Super_Resolution/Figure6.png" style="width:100%">
  <figcaption>
  Fig.5 - Meta upscale module. The blue boxes denote the projection patch, and the green boxes and lines indicate the convolution operation with predicted weights.
  </figcaption>
</p>

Specifically, for each target position on the HR images, this module project it to a small patch on the LR feature maps (i.e., *k* x *k* x *c<sub>in</sub>*), predicts convolution weights (i.e., *k* x *k* x *c<sub>in</sub>* x *c<sub>out</sub>*) according to the projection offsets and the scaling factor by dense layers and perform convolution. In this way, the meta upscale module can continuously zoom in it with arbitrary factors by a single model. And due to the large amount of training data (multiple factors are simultaneously trained), the module can exhibit comparable or even better performance on fixed factors.  

Although this module needs to predict weights during inference, the execution time of the upsampling module only accounts for about 1% of the time of feature extraction. However, this method predicts a large number of convolution weights for each target pixel based on several values independent of the image contents, so the prediction result may be unstable and less efficient when faced with larger magnifications.  


Nowadays, These learning-based layers have become the most widely used upsampling methods. Especially in the post-upsampling framework, these layers are usually used in the final upsampling phase for reconstructing HR images based on high-level representations extracted in low-dimensional space, and thus achieve end-to-end SR while avoiding overwhelming operations in high-dimensional space.  

<br>

#### (3) Network Design  

In the super-resolution field, researchers apply all kinds of network design strategies on top of the four SR frameworks to construct the final networks. In this section, we decompose these networks to essential principles or strategies for network design, introduce them and analyze the advantages and limitations one by one.  

<p>
  <img src="/assets/images/blog/Super_Resolution/Figure7.png" style="width:100%">
  <figcaption>
  Fig.6 - Network design startegies.
  </figcaption>
</p>


##### 3.1 Residual Learning

Before, ResNet was proposed for learning residuals instead of a through mapping. The residual learning strategies can be roughly divided into global and local residual learning.  

**Global Residual Learning**  
Since the image SR that the input image is highly correlated with the target image, researchers try to learn only the residuals between them, namely global residual learning. In this cases, it avoids learning a complicated transformation from a complete image to another, instead only requires learning a residual map to restore the missing high-frequency details. Since the residuals in most regions are close to zero, the model complexity and learning difficulty are greatly reduced.  

**Local Residual Learning**  
The local residual learning is used to alleviate the degradation problem caused by ever-increasing network depths, reduce training difficulty and improve the learning ability.  

In practice, the above methods are both implemented by shortcut connections (often scaled by a small constant) and element-wise addition, while the difference is that the former directly connects the input and output images, while the latter usually adds multiple shortcuts between layers with different depths inside the network.  


##### 3.2 Recursive Learning  

In order to learn higher-level features without introducing overwhelming parameters, recursive learning, which means applying the same modules multiple times in a recursive manner, is introduced into the SR field.  
1. DRCN: 16-recursive employs a single convolutional layer as the recursive unit.  
2. DRRN: 25-recursive ResBlock (better performance than 17-ResBlock baseline)  
3. MemNet: Based on memory block (composed of a 6-recursive ResBlock, the outputs of every recursion are concat and go through an extra 1x1 convolution)  
<p>
  <img src="/assets/images/blog/Super_Resolution/Figure8.png" style="width:100%">
  <figcaption>
  Fig.7 - Memory block.
  </figcaption>
</p>
+) CARN, DSRN, ...  

In general, the recursive learning can indeed learn more advanced representations without introducing excessive parameters, but still can't avoid high computational costs. And it inherently brings vanishing or exploding gradient problems, consequently some techniques such as residual learning and multi-supervision are often integrated with recursive learning for mitigating these problems.  


##### 3.3 Multi-path Learning  

Multi-path learning refers to passing features through multiple paths, which perform different operations, and fusing them back for providing better modelling capabilities. Specifically, it could be divided into global, local and scale-specific multi-path learning.  

**Global Multi-path Learning**  
Global multi-path learning refers to making use of multiple paths to extract features of different aspects of the images. These paths can cross each other in the propagation and thus greatly enhance the learning ability.  
1. LapSRN: A feature extraction path + HR images reconstruction path  
2. DSRN: Two information extraction path in low-dimensional and high-dimensional space  
3. Pixel Recursive SR: A conditioning path (capture the global structure) + A prior path (capture the serial dependence of generated pixels)  
+) [Ren et al.](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/papers/Ren_Image_Super_Resolution_CVPR_2017_paper.pdf) employ multiple paths with unbalanced structures to perform upsampling and fuse them at the end of the model.  

**Local Multi-path Learning**  
Through local multi-path learning, the SR models can better extract image features from multiple scales and further improve performance. (e.g., MSRN)
- MSRN: In a block, two conv layers with kernel size 3x3 and 5x5 are adopted. then the outputs are concatenated and go through the same operations again, and finally an extra 1x1 convolution is applied. A shortcut connects the input and output by element-wise addition.  

**Scale-specific Multi-path Learning**  
Considering that SR models for different scales need to go through similar feature extraction, EDSR was proposed, which has scale-specific multi-path learning to cope with multi-scale SR with a single network. To be concrete, they share the principal components of the model (i.e., the intermediate layers for feature extraction), and attach scale-specific pre-processing paths and upsampling paths at the beginning and the end of the network, respectively. During training, only the paths corresponding to the selected scale are enabled and updated. In this way, the proposed MDSR greatly reduce the model size by sharing most of the parameters for different scales and exhibits comparable performance as single-scale models. The similar scale-specific multi-path learning is also adopted by CARN and ProSR.  


##### 3.4 Dense Connections  

For the sake of fusing low-level and high-level features to provide richer information for reconstructing high-quality details, dense connections are introduced into the SR field. For each layer in a dense block, the feature maps of all preceding layers are used as inputs, and its own feature maps are used as inputs into all subsequent layers, so that it leads to *l*. The dense connections not only help alleviate gradient vanishing, enhance signal propagation and encourage feature reuse, but also substantially reduce the model size by employing small growth rate (i.e., number of channels in dense blocks) and squeezing channels after concatenating all input feature maps.  
- SRDenseNet, MemNet, CARN, RDN, ESRGAN, DBPN  


##### 3.5 Attention Mechanism!!!  

**Channel Attention**  
Considering the interdependence and interaction of the feature representations between different channels, a "squeeze-and-excitation" block is proposed to improve learning ability by explicitly modelling channel interdependence. In this block, each input channel is squeezed into a channel descriptor using global average pooling (GAP), then these descriptors are fed into two dense layers to produce channel-wise scaling factors for input channels.  
- RCAN, SOCA  

**Non-local Attention**  
Most existing SR models have very limited local receptive fields. However, some distant objects or textures may be very important for local patch generation. So that [Zhang et al.](https://arxiv.org/pdf/1903.10082.pdf) propose local and non-local attention blocks to extract features that capture the long-range dependencies between pixels. Specifically, they propose a trunk branch for extracting features, and a (non-) local mask branch for adaptively rescaling features of trunk branch. Among them, the local branch employs an encoder-decoder structure to learn the local attention, while the non-local branch uses the embedded Gaussian function to evaluate pairwise relationships between every two position indices in the feature maps to predict the scaling weights. Through this mechanism, the proposed method captures the spatial attention well and further enhances the representation ability.  


##### 3.6 Advanced Convolution  

Since convolution operations are the basis of deep neural networks, researchers also attempt to improve convolution operations for better performance or greater efficiency.  

**Dilated Convolution**  
It is well known that the contextual information facilitates generating realistic details for SR. By replacing the common convolution by dilated convolution in SR models can increase the receptive field over twice and achieve much better performance. ([Zhang et al.](https://arxiv.org/pdf/1704.03264.pdf))  

**Group Convolution**  
The group convolution much reduces the number of parameters and operations at the expense of a little performance loss (IDN, CARN-M)  

**Depthwise Separable Convolution**  
[MobileNet](https://arxiv.org/pdf/1704.04861.pdf) propose depthwise separable convolution. it consists of a factorized depthwise convolution and a pointwise convolution (i.e., 1x1 convolution), and thus reduces plenty of parameters and operations at only a small reduction in accuracy. (+ [Nie et al](https://arxiv.org/pdf/1810.01641.pdf))


##### 3.7 Region-recursive Learning  

Most SR models treat SR as a pixel-independent task and thus cannot source the interdependence between generated pixels properly. Pixel recursive SR first propose pixel recursive learning to perform pixel-by-pixel generation, by employing two networks to capture global contextual information and serial generation dependence, respectively. In this way, the proposed method synthesizes realistic hair and skin details on super-resolving very low-resolution face images (e.g., 8x8) and far exceeds the previous methods on MOS testing.  

The Attention-FH also adopts this strategy by resorting to a recurrent policy network for sequentially discovering attended patches and performing local enhancement. In this way, it is capable of adaptively personalizing an optimal searching path for each image according to its own characteristic, and thus fully exploits the global intra-dependence of images.  

Although these methods show better performance to some extent, the recursive process requiring a long propagation path greatly increases the computational cost and training difficulty, especially for super-resolving HR images.


##### 3.8 Pyramid Pooling  

[Zhao et al.](https://arxiv.org/pdf/1612.01105.pdf) propose the pyramid pooling module to better utilize global and local contextual information. Specifically, for feature maps sized *h* x *w* x *c*, each feature map is divided into *M* x *M* bins, and goes through global average pooling, resulting in *M* x *M* x *c* outputs. Then a 1x1 convolution is performed for compressing the outputs to a single channel. After that, the low-dimensional feature map is upsampled to the same size as the original feature map via bilinear interpolation. By using different M, the module integrates global as well as local contextual information effectively. By incorporating this module, the proposed EDSR-PP model further improve the performance over baseline.  


##### 3.9 Wavelet Transformation  

The wavelet transformation (WT) is a highly efficient representation of images by decomposing the image signal into high-frequency sub-bands denoting texture details and low-frequency sub-bands containing global topological information. [Bae et al.](https://arxiv.org/pdf/1611.06345.pdf) take sub-bands of interpolated LR wavelet as input and predict residuals of corresponding HR sub-bands. WT and inverse WT are applied for decomposing the LR input and reconstructing the HR output, respectively. Due to the efficient representation by wavelet transformation, the models using this strategy often much reduce the model size and computational cost, while maintain competitive performance.  
- DWSR, Wavelet-SRNet, MWCNN  


##### 3.10 Desubpixel  

In order to speed up the inference speed, [Vu et al.](https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Vu_Fast_and_Efficient_Image_Quality_Enhancement_via_Desubpixel_Convolutional_Neural_ECCVW_2018_paper.pdf) propose to perform the time-consuming feature extraction in a lower-dimensional space, and propose desubpixel, an inverse of the shuffle operation of sub-pixel layer. Specifically, the desubpixel operation splits the images spatially, stacks them as extra channels and thus avoids loss of information. In this way, they downsample input images by desubpixel at the beginning of the model, lean representations in a lower-dimensional space, and upsample to the target size at the end. The proposed model achieves the best scores in the PIRM Chalenge on Smartphones with very high-speed inference and good performance.  


##### 3.11 xUnit  

In order to combine spatial feature processing and nonlinear activations to learn complex features more efficiently, [Kligvasser et al.](https://arxiv.org/pdf/1711.06445.pdf) propose xUnit for learning a spatial activation function. Specifically, the ReLU is regarded as determining a weight map to perform element-wise multiplication with the input, where the xUnit directly learn the weight map through convolution and Gaussian gating. Although the xUnit is more computationally demanding, due to its dramatic effect on the performance, it allows greatly reducing the model size while matching the performance with ReLU. In this way, the authors reduce the model size by nearly 50% without any performance degradation.  



#### (4) Learning Strategies  

##### 4.1 Loss Functions  
In the super-resolution field, loss functions are used to measure reconstruction error and guide the model optimization. In early times, researchers usually employ the pixelwise L2 loss, but later discover that it cannot measure the reconstruction quality very accurately. Therefore, a variety of loss function are adopted for better measuring the reconstruction error and producing more realistic and higher-quality results. In this section, we'll take  a close look at the loss functions used widely.  

**Pixel Loss**  
Pixel loss measures pixel-wise difference between two images and mainly includes L1 loss (MAE, Mean Absolute Error) and L2 loss (MSE, Mean Squared Error):  
<p>
  <img src="/assets/images/blog/Super_Resolution/Equation8.png" style="width:50%">
</p>
where *h*, *w* and *c* are the height, width and number of channels of the evaluated images, respectively. In addition, there is a variant of the pixel L1 loss, namely Charbonnier loss, given by:  
<p>
  <img src="/assets/images/blog/Super_Resolution/Equation9.png" style="width:50%">
</p>
where &epsilon; is a constant (e.g., 10<sup>-3</sup>) for numerical stability.  

The pixel loss constrains the generated HR image *I&#770;* to be close enough to the ground truth I on the pixel values. Comparing with L1 loss, L2 loss penalizes larger errors but is more tolerant to small errors, and thus often results in too smooth results. In practice, the L1 loss shows improved performance and convergence over L2 loss.  

Since the definition of PSNR is highly correlated with pixel-wise difference and minimizing pixel loss directly maximize PSNR, the pixel loss gradual becomes the most widely used loss function. However, since the pixel loss actually doesn't take image quality (e.g., perceptual quality, textures) into account, the results often lack high-frequency details and are perceptually unsatisfying with over-smooth textures.  

**Content Loss**  
In order to evaluate perceptual quality of images, the content loss is introduced into SR. Specifically, it measures the semantic differences between images using a pre-trained image classification network. Denoting this network as &phi; and the extracted high-level representations on *l*-th layer as *&phi;<sup>(l)</sup>(I)*, the content loss is indicated as the Euclidean distance between high-level representations of two images, as follows:  
<p>
  <img src="/assets/images/blog/Super_Resolution/Equation10.png" style="width:50%">
</p>
where *h<sub>l</sub>*, *w<sub>l</sub>* and *c<sub>l</sub>* are the height, width and number of channels of the representations on layer *l*, respectively.  

Essentially the content loss transfers the learned knowledge of hierarchical image features from the classification network &phi; to the SR network. In contrast to the pixel loss, the content loss encourages the output image *I&#770;* to be perceptually similar to the target image I instead of forcing them to match pixels exactly. Thus it produces visually more perceptible results and is also widely used in this field, where the VGG and ResNet are the most commonly used per-trained CNNs.  


**Texture Loss**  
On account that the reconstructed image should have the same style (e.g., colors, textures, contrast) with the target image, the texture loss (a.k.a. style reconstruction loss) is introduced into SR. The image texture is regarded as the correlations between different feature channels and defined as the Gram matrix *G<sup>(l)</sup> &isin; c<sub>l</sub>* x *c<sub>l</sub>*, where  *G<sup>(l)</sup><sub>ij</sub>* is the inner product between the vectorized feature maps *i* and *j* on layer *l*:  
<p>
  <img src="/assets/images/blog/Super_Resolution/Equation11.png" style="width:50%">
</p>
where *vec(.)* denotes the vectorization operation, and *&phi;<sup>(l)</sup><sub>i</sub>(I)* denotes the *i*-th channel of the feature maps on layer *l* of image I. Then the texture loss is given by:  
<p>
  <img src="/assets/images/blog/Super_Resolution/Equation12.png" style="width:50%">
</p>
By employing texture loss, the EnhanceNet creates much more realistic textures and produces visually more satisfactory results. Despite this, determining the patch size to match textures is still empirical. Too small patches lead to artifacts in textured regions, while too large patches lead to artifacts throughout the entire image because texture statistics are averaged over regions of varying textures.  


**Adversarial Loss**  



In this post, we looked over an overview of the super resolution. In the next post, we will look into the Super resolution in more detail.
