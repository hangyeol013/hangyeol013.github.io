---
title:  "Deep Learning survey_3.Convolutional Neural Network (CNN)"
search: true
categories:
  - Deep learning
summary: This post is the third part (CNN section) of summary of a survey paper.
toc: true
toc_sticky: true
header:
  teaser: /assets/images/thumbnails/thumb_basic.jpg
last_modified_at: 2021-02-22T08:06:00-05:00
---


This post is the third part (CNN section) of summary of a survey paper
[A state-of-the Art survey on Deep learning theory and architecture](https://www.mdpi.com/2079-9292/8/3/292).  
This part is the basics of CNNs so, if you've already known them, you can skip this part.


### 3.1. CNN overview  

1988 (Fukushima): the CNN network was first proposed but was not widely used due to limits of computation hardware for training the network.  
1990s (LeCun): A gradient-based learning algorithm was applied to CNNs and obtained successful results for the handwritten digit classification problem.  

##### *Several advantages of CNNs over DNNs:  
1) Being more like the human visual processing system.  
2) Being highly optimized in the structure for processing 2D and 3D images.  
3) Being `effective at learning and extracting abstractions of 2D features`.  
4) The max pooling layer of CNNs is effective in absorbing shape variations.  
5) CNNs have `significantly fewer parameters than a fully connected network` of similar size (composed of sparse connections with tied weights)  
6) CNNs are trained with the gradient-based learning algorithm and suffer less from the diminishing gradient problem.  


Figure 1 shows the overall architecture of CNNs consists of two main parts: **feature extractors and a classifier**.  
<br>
**Feature extraction layers)**  
Here are from a paper ['A detailed review of feature extraction in image processing systems'](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6783417).  
- Feature extraction describes `the relevant shape information contained in a pattern` so that the task of classifying the pattern is made easy by a formal procedure.  
- In pattern recognition and in image processing, feature extraction is a special form of dimensionality reduction.  
- `The main goal of feature extraction` is to obtain `the most relevant information from the original data` and `represent that information in a lower dimensionality space`. (the extraction of the relevant information that characterizes each class.)  
- When the input data to an algorithm is too large to be processed and it is suspected to be redundant (much data, but not much information) then the input data will be transformed into a reduced representation set of features (also named feature vectors)  
- Transforming the input data into the set of feature is called feature extraction.  
- Feature selection (extraction) is critical to the whole process since the classifier will not be able to recognize from poorly selected features.  


*** The CNN architecture consists of a combination of three types of layers:**  
Convolution, max-pooling and Classification.  

<p>
  <img src="/assets/images/blog/DL_survey_03.CNN/Figure1.png" style="width:80%">
  <figcaption>Fig.1 - The overall architecture of CNNs.</figcaption>
</p>

- Each plane of a layer is usually derived from the combination of one or more planes of previous layers.  
- The nodes of a plane are connected to the small region of each connected planes of the previous layer.  
- `Each node` of the convolution layer `extracts the features from the input images` by convolution operations on the input nodes.
- As the features propagate to the highest layer or level, `the dimensions of features are reduced` depending on the size of the kernel for the convolutional and max-pooling operations respectively.  
- However, `the number of feature maps` usually `increased` for `representing better features` of the input images for ensuring classification accuracy.  
- The output of the last layer of the CNN is used as the input to a fully connected network which is called classification layer.  
- The fully connected layers are expensive in terms of network or learning parameters, so nowadays, there are several new techniques including average pooling and global average pooling that is used as an alternative of fully-connected networks.  
- The score of the respective class is calculated in the top classification layer using a soft-max layer. Based on the highest score, the classifier gives output for the corresponding classes.  



Later, we will discuss about mathematical details on different layers of CNNs (Convolutional layer, pooling layer, classification layer).


### 3.1.1. Convolutional Layer  

- In this layer, feature maps from previous layers are `convolved with learnable kernels`.  
- The output of the kernels goes through a linear or non-linear activation function, such as signoid, tanh, softmax, ReLU, ...) to form the output feature maps.  

**+) what if do not use any activation function?**  
Consider a two layer neural network. If there isn't any activation function, the calculation from two layers equivalent to a single layer neural network (you can check this with a few equations). It is well known that single layer neural network cannot even solve 'simple' problem like XOR problem.  
Introduction non-linear activation functions between the layers allows for the network to solve a larger variety of problems.

- Each of the output feature maps can be combined with more than one input feature map.  

<p>
  <img src="/assets/images/blog/DL_survey_03.CNN/equation1.png" style="width:25%">
</p>

*x<sup>l</sup><sub>j</sub>*: the output of the current layer.  
*x<sup>l-1</sup><sub>i</sub>*: the previous layer output.  
*k<sup>l</sup><sub>ij</sub>*: the kernel for the present layer. (the input maps will be convolved with distinct kernels to generate the corresponding output maps)  
*b<sup>l</sup><sub>j</sub>*: the biases for the current layer. (for each output map, an additive bias b is given)  
*M<sub>j</sub>*: a selection of input maps.  
*f*: activation function (such as sigmoid, tanh, softmax, ReLU, ...)  


<p>
  <img src="/assets/images/blog/DL_survey_03.CNN/Figure14.png" style="width:80%">
  <figcaption>Fig.2 - Convolution operation example.</figcaption>
</p>


- For each placement of a given kernel, a multiplication operation is performed between the input section and the kernel, with the bias summed to the result.  
- This produces a feature map containing the convolved result. The feature maps were typically passed through an activation function to provide input for the subsequent layer.  
- Generally, the number of kernel channels is always identical to the number of input channels.  


### 3.1.2. Sub-sampling Layer  

- The subsampling layer performs `the down sampled operation` on the input maps.
- This is commonly known as `the pooling layer`.  
- In this layer, the number of input and output feature maps does not change.
- Due to the down sampling operation, `the size of each dimension` of the output maps will be `reduced`, depending on the size of the down sampling mask.  

<p>
  <img src="/assets/images/blog/DL_survey_03.CNN/equation2.png" style="width:20%">
</p>

*down(.)*: a sub-sampling function.
- Two types of operations are mostly performed in this layer: `average pooling` (selects the average value) or `max-pooling` (selects the highest value)  
- Some alternative sub-sampling layers have been proposed, such as `fractional max-pooling` and `sub-sampling with convolution`.  


### 3.1.3. Classification Layer  

- The `fully connected layer` `computes the score of each class` from the extracted features from a convolutional layer in the preceding steps.
- The fully connected feed-forward neural layers are used as a soft-max classification layer.  
- In most cases, two or four layers of layers are incorporated in the network model.  
- As the fully connected layers are `expensive in terms of computation`, alternative approaches have been proposed. (`global average pooling layer`, `average pooling layer`)  

<p>
  <img src="/assets/images/blog/DL_survey_03.CNN/Figure2.png" style="width:50%">
  <figcaption>Fig.1 - The basic operations in the convolution and sub-sampling of an input image.</figcaption>
</p>



### 3.1.4. Network Parameters and Required Memory for CNN  
<br>
The number of computational parameters is an important metric to measure `the complexity` for a deep learning model.  
`The size of feature maps` can be formulated as follows:

<p>
  <img src="/assets/images/blog/DL_survey_03.CNN/equation3.png" style="width:25%">
</p>

*N*: the dimensions of the input feature maps
*F*: the dimensions of the filters or the receptive field
*M*: the dimensions of output feature maps
*S*: the stride length


**Padding** is typically applied during the convolution operations to ensure the input and output `feature map` have `the same dimensions`. The amount of padding depends on `the size of the kernel`.  
<br>
Several criteria are considered for comparing the models, in most of the cases, `the number of network parameters` and `the total amount of memory` are considered.  

<br>
The number of parameters of *l<sup>th</sup>* layer is the calculated based on the following equation:

<p>
  <img src="/assets/images/blog/DL_survey_03.CNN/equation4.png" style="width:40%">
</p>

If bias is added with the weights, then the above equation can be written as follows:

<p>
  <img src="/assets/images/blog/DL_survey_03.CNN/equation5.png" style="width:40%">
</p>

* The equation need to be revised to Parm<sub>l</sub> = *(F x F x (FM<sub>l-1</sub> + 1)) x FM<sub>l</sub>*  
(**this equation need to be checked**)

*FM<sub>l</sub>*: the total number of output feature maps  
*FM<sub>l-1</sub>*: the total number of input feature maps or channels

<br>

In the next post, I will discuss about the popular CNN architectures.
